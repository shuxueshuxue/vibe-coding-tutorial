
## 1. Learn By Doing, but in LLM era
（LLM 时代，正确的的“做中学”是怎样的？）

在 [现代软件工程 习而学的软件工程教育](https://www.cnblogs.com/xinz/archive/2012/01/08/2316717.html) 中，邹欣老师提出了要进行具体能实践的，能看到结果的软件工程教育，在做中学。
按我的理解，这是通过能持续带来反馈的“做”，来倒逼“学”，给学习以动力。

这基于一个关键假设：
学 -> 做，中间满足强依赖关系。也即你不学，就没法做。
然而在当前的 LLM 时代，至少对于大部分水平不高的项目，我们发现中间的箭头弱化，
学/AI ~> 做，即若你不学，也能通过 LLM 凑合出一能用的版本。
**这时候“做”对学的倒逼作用就减少了。甚至消失。**

这并不是说 LLM 可以 cover 所有课上能布置的项目，目前的 LLM 纯靠自身依然无法产出高质量的代码，然而使用者总可以像掷骰子一样，重复地 prompt 它，在不加以引导的情况下让代码可以 work.
所以就会出现这样的情况，当程序出错时，学习者内心有两种声音，“我需要了解一下 LLM 生成的这段代码的原理，然后引导 LLM 生成更好的方案” or “我需要让它再试几次，总能成功吧”。倘若此时时间紧张，甚至明天就是 ddl，他就很可能会选择后者，用一段自己也没有掌控的代码提交了作业。

这并非只发生在课程学习中，我曾见过这样写实际业务代码的工程师。

对此，我自己找的解决方法是，有意识地放缓 vibe coding 的节奏，尤其是在架构设计阶段，确保自己完全弄清楚了整个技术栈。不停地 sanity check 自己和 AI，比较不同方案的优劣。
另外，有意识地停止写代码，去看一些高质量的博客文章（比如 hacknews 上的热门文章）和编程语言教程，确保有足够的输入。正所谓思而不学则殆，虽然现在检索东西很方便，我还是喜欢背诵记忆一些知识点，例如这套 [anki deck ](https://ido777.github.io/system-design-primer-update/en/sd_anki_flashcards.html)

## 2. 适合 vibe coding 的团队开发模式是怎样的？

[现代软件工程讲义 4 团队和流程](https://www.cnblogs.com/xinz/archive/2011/10/07/2200511.html) 一节介绍了各式各样的团队开发流程，包括主治医生模式，社区模式，特工模式等。

之前在一家做 LLM 应用的初创公司（规模大概50人）中工作时，我们主张每个人都是全栈，既能做自己擅长的部分，又能做任何项目当前需要的部分（保证快速交付）。
由于 LLM 能力的加强，“特工模式”或许在逐渐消失。

我这里想指出大家经常忽略的一个 vibe coding 的优势：合理的 context 保存可以让中途加入的开发者快速了解项目当前的进展，以及让合作者互相知道对方做了什么，让合作中的沟通更快捷。这种“快速启动”的便利对于个人也有用：我可以一个人并行开发多个项目，不断地 context switching，而不用担心忙完一个忘了另一个的细节。所以 vibe coding 远不止有“让简单的项目开发更快”这样的低级功能。AI 作为桥梁，能让进行复杂项目开发的程序员之间的沟通更高效。目前有很多产品，例如 OpenAI 的 Codex 就在做这样的探索。

## 3. LLM 时代的软件测试如何进行？

在 [这一节](https://www.cnblogs.com/xinz/p/3857368.html) 中，邹欣讨论了开发团队中测试的角色。
文章中提到 Ad hoc 测试的特点是"随机进行的、探索性的"，但同时指出它"不能自动化"、"不可重复"。在传统软件工程中，这确实是 Ad hoc 测试的根本局限。

但现在有了一个有趣的悖论：我们可以用 LLM 来大规模生成"非重复性"的测试用例。它的价值在于能够以工业化的规模进行创造性破坏，发现我们从未想过要测试的场景。

我们甚至可以把测试当成开发本身，依赖各种 Code Agent 来进行测试驱动开发。参见 [该视频](https://www.bilibili.com/video/BV13MpCzjEa3/?vd_source=ee5d436c57dcb81255f798532317c6db)

<img width="600" height="300" alt="image" src="https://github.com/user-attachments/assets/e5e305a1-4d97-4c79-840b-cdb927a3329f" />

但实际应用中有很多的挑战：
1. 我们依然需要人类工程师对测试代码本身的质量进行评估，以及和用户需求对齐
2. 如何管理 LLM 的 context，使得测试本身不造成过大的（时间、API 费用）成本
3. 如何对各种 ad hoc 测试的重要程度进行排序，测试完成后生成人类可读的报告

## 4. 关于 AI 生成代码的绩效评估

[Vibe Coding Cleanup as a Service](https://donado.co/en/articles/2025-09-16-vibe-coding-cleanup-as-a-service/)
根据最新的行业报告，92% 的开发者在使用 AI 编程工具，但 AI 生成的代码带来了严重的质量问题 - 市场上已经出现了专门的"Vibe Coding Cleanup"服务，专业人员收费 $200-400/小时 来重构 AI 生成的代码。

在这种环境下，如何评估程序员的真实贡献？

- 一个程序员快速用 AI 生成了功能，但后期需要 $200/小时 的专家来清理
- 另一个程序员前期慢一些，但生成的代码后续维护起来工作量小

从 ROI 角度看，后者明显更有价值，但传统的短期绩效评估很难体现这一点。我们如何能量化代码的"质量"？
